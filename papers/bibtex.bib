@InProceedings{guzman2015-ACL,
  author    = {Guzm\'{a}n, Francisco  and  Joty, Shafiq  and  M\`{a}rquez, Llu\'{i}s  and  Nakov, Preslav },
  title     = {Pairwise Neural Machine Translation Evaluation},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and The 7th International Joint Conference of the Asian
            Federation of Natural Language Processing ({ACL}'15)},
  month     = {July},
  year      = {2015},
  address   = {Beijing, China},
  publisher = {Association for Computational Linguistics},
  pages     = {805--814},
  url       = {http://www.aclweb.org/anthology/P15-1078},
  Abstract = {We present a novel framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is compacted into relatively small distributed vector representations, and fed into a multi-layer neural network that models the interaction between each of the hypotheses and the reference, as well as between the two hypotheses. These compact representations are in turn based on word and sentence embeddings, which are learned using neural networks. The framework is flexible, allows for efficient learning and classification, and yields correlation with humans that rivals the state of the art.}
}


@InProceedings{guzman-nakov-vogel:2015:CoNLL,
  author    = {Guzm\'{a}n, Francisco  and  Nakov, Preslav  and  Vogel, Stephan},
  title     = {Analyzing Optimization for Statistical Machine Translation: MERT Learns Verbosity, PRO Learns Length},
  booktitle = {Proceedings of the Nineteenth Conference on Computational Natural Language Learning},
  month     = {July},
  year      = {2015},
  address   = {Beijing, China},
  publisher = {Association for Computational Linguistics},
  pages     = {62--72},
  url       = {http://www.aclweb.org/anthology/K15-1007},
  abstract = {We study the impact of source length and verbosity of the tuning dataset on the performance of parameter optimizers such as MERT and PRO for statistical machine translation. In particular, we test whether the verbosity of the resulting translations can be modified by varying the length or the verbosity of the tuning sentences. We find that MERT learns the tuning set verbosity very well, while PRO is sensitive to both the verbosity and the length of the source sentences in the tuning set; yet, overall PRO learns best from high-verbosity tuning datasets.
Given these dependencies, and potentially some other such as amount of reordering, number of unknown words, syntactic complexity, and evaluation measure, to mention just a few, we argue for the need of controlled evaluation scenarios, so that the selection of tuning set and optimization strategy does not overshadow scientific advances in modeling or decoding. In the mean time, until we develop such controlled scenarios, we recommend using PRO with a large verbosity tuning set, which, in our experiments, yields highest BLEU across datasets and language pairs.}
}

@InProceedings{guzman-EtAl:2015:WMT,
  author    = {Guzm\'{a}n, Francisco  and  Abdelali, Ahmed  and  Temnikova, Irina  and  Sajjad, Hassan  and  Vogel, Stephan},
  title     = {How do Humans Evaluate Machine Translation},
  booktitle = {Proceedings of the Tenth Workshop on Statistical Machine Translation},
  month     = {September},
  year      = {2015},
  address   = {Lisbon, Portugal},
  publisher = {Association for Computational Linguistics},
  pages     = {457--466},
  url       = {http://aclweb.org/anthology/W15-3059},
  Abstract  = {In this paper, we take a closer look at the MT evaluation process from a glass-box perspective using eye-tracking. We analyze two aspects of the evaluation task: the background of evaluators (monolingual or bilingual) and the sources of information available, and we evaluate them using time and consistency as criteria. Our findings show that monolinguals are slower but more consistent than bilinguals, especially when only target language information is available. When exposed to various sources of information, evaluators in general take more time and in the case of monolinguals, there is a drop in consistency. Our findings suggest that to have consistent and cost effective MT evaluations, it is better to use monolinguals with only target language information.}
}

@InProceedings{sajjad-EtAl:2016:N16-1,
  author    = {Sajjad, Hassan  and  Guzm\'{a}n, Francisco  and  Durrani, Nadir  and  Abdelali, Ahmed  and  Bouamor, Houda  and  Temnikova, Irina  and  Vogel, Stephan},
  title     = {Eyes Don't Lie: Predicting Machine Translation Quality Using Eye Movement},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2016},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  pages     = {1082--1088},
  url       = {http://www.aclweb.org/anthology/N16-1125}
}

@InProceedings{abdelali-durrani-guzman:2016:N16-3,
  author    = {Abdelali, Ahmed  and  Durrani, Nadir  and  Guzm\'{a}n, Francisco},
  title     = {iAppraise: A Manual Machine Translation Evaluation Environment Supporting Eye-tracking},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations},
  month     = {June},
  year      = {2016},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  pages     = {17--21},
  url       = {http://www.aclweb.org/anthology/N16-3004}
}


